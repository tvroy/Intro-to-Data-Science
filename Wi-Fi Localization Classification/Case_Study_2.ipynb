{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this case study, you will perform a classification task on a WiFi dataset, and also explore the question, \"Is more data useful for a classification task?\"\n",
    "\n",
    "The dataset you will use can be found on: https://archive.ics.uci.edu/ml/datasets/ujiindoorloc .\n",
    "\n",
    "**\\[Step 1\\]** Once you examine the data sets, you will find that there is a training set and a validation set. You can use them to build your classification model. You might need to determine what are your features and targets. You can also do some engineering on features and targets if necessary.\n",
    "\n",
    "**\\[Step 2\\]** But, which algorithm should you use with your model? You can refer to the scikit-learn cheat sheet: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html, and try three algorithms. Some suggestions are: LinearSVC, Logistic Regression, KNN classifier, SVC, Random Forest (as an example of Ensemble Learning) etc. Perform one experiment using each and observe the performance of each model. Note which is the best performing model.\n",
    "\n",
    "**\\[Step 3\\]** Once the previous step is done, observe if more data is useful for a classification task using the best performing model from the previous step. For this, randomly select 20% of the training samples, but keep the size of the validation set the same. Note the performance. Then also try with 40%, 60%, 80% and 100% of the training samples. Perform three experiments for each selection. This means, for 20% you will do three experiments, 40% three experiments etc. Find the average of three experiments for each selection and plot them using a chart of your choice.\n",
    "\n",
    "**\\[Step 4\\]** Publish your finding in presentation slides. Like case study 1, three of you will be randomly chosen to present your work in front of the class. The slides should inform the audience about:\n",
    "\n",
    "* the objective of the case study\n",
    "* the data (features and targets)\n",
    "* things you have done (e.g. why you selected a specific classification model)\n",
    "* your findings.\n",
    "\n",
    "\n",
    "**Things to note**:\n",
    "\n",
    "* **Type of task**: classification\n",
    "* **Features**: you choose.\n",
    "* **Feature engineering**: You are welcome to do so.\n",
    "* **Target**: Use a combination of features to learn from and identify the location. Ignore the SPACEID column.\n",
    "\n",
    "* In some cases, Normalization may result in reduced accuracy.\n",
    "* You must write enough comments so that anybody with some programming knowledge can understand your code.\n",
    "\n",
    "**Grading Criteria**:\n",
    "\n",
    "* [15 + 15] Data set preparation: Choosing your $X$ and $y$. Feature Engineering.\n",
    "* [15 + 15 + 15] Three experiments using three algorithms.  \n",
    "* [15] Observing the effects of more data using five sets of random samples of different sizes from the training set. \n",
    "* [10] Presentation slides\n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "Put the Jupyter Notebook file and the .csv file in a folder. Then convert your presentation slides in to a PDF file and put it in the same folder. Zip the folder. After zipping, it should have the extension .zip. The name of the .zip file should be firstname_lastname_casestudy_2.zip . Upload the .zip file on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "#import training and validation datasets\n",
    "df_train = pd.read_csv('./trainingData.csv')\n",
    "df_val = pd.read_csv('./validationData.csv')\n",
    "\n",
    "#drop columns I will not be using as features. \n",
    "df_train = df_train.drop(columns = ['SPACEID', 'PHONEID', 'TIMESTAMP', 'USERID', 'RELATIVEPOSITION', 'LONGITUDE', 'LATITUDE'])\n",
    "df_val = df_val.drop(columns = ['SPACEID', 'PHONEID', 'TIMESTAMP', 'USERID', 'RELATIVEPOSITION', 'LONGITUDE', 'LATITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm no missing values\n",
    "print(df_train.isnull().values.any())\n",
    "print(df_val.isnull().values.any())\n",
    "\n",
    "#Find out how many combinations of buildingid and floor there are\n",
    "print(df_train.groupby(['BUILDINGID','FLOOR']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a single target variable\n",
    "#Assign a value to each possible building/floor combination and add it to a single list of location identifiers\n",
    "\n",
    "ft = df_train['FLOOR'].tolist()\n",
    "bt = df_train['BUILDINGID'].tolist()\n",
    "loct = []\n",
    "z = zip(bt, ft)\n",
    "\n",
    "for e1, e2 in z:\n",
    "    if e1 == 0 and e2 == 0:\n",
    "        loct.append(0)\n",
    "    elif e1 == 0 and e2 == 1:\n",
    "        loct.append(1)\n",
    "    elif e1 == 0 and e2 == 2:\n",
    "        loct.append(2)\n",
    "    elif e1 == 0 and e2 == 3:\n",
    "        loct.append(3)\n",
    "    elif e1 == 1 and e2 == 0:\n",
    "        loct.append(4)\n",
    "    elif e1 == 1 and e2 == 1:\n",
    "        loct.append(5)\n",
    "    elif e1 == 1 and e2 == 2:\n",
    "        loct.append(6)\n",
    "    elif e1 == 1 and e2 == 3:\n",
    "        loct.append(7)\n",
    "    elif e1 == 2 and e2 == 0:\n",
    "        loct.append(8)\n",
    "    elif e1 == 2 and e2 == 1:\n",
    "        loct.append(9)\n",
    "    elif e1 == 2 and e2 == 2:\n",
    "        loct.append(10)\n",
    "    elif e1 == 2 and e2 == 3:\n",
    "        loct.append(11)\n",
    "    elif e1 == 2 and e2 == 4:\n",
    "        loct.append(12)\n",
    "\n",
    "#do the same for the validation set\n",
    "fv = df_val['FLOOR'].tolist()\n",
    "bv = df_val['BUILDINGID'].tolist()\n",
    "locv = []\n",
    "z = zip(bv, fv)\n",
    "\n",
    "for e1, e2 in z:\n",
    "    if e1 == 0 and e2 == 0:\n",
    "        locv.append(0)\n",
    "    elif e1 == 0 and e2 == 1:\n",
    "        locv.append(1)\n",
    "    elif e1 == 0 and e2 == 2:\n",
    "        locv.append(2)\n",
    "    elif e1 == 0 and e2 == 3:\n",
    "        locv.append(3)\n",
    "    elif e1 == 1 and e2 == 0:\n",
    "        locv.append(4)\n",
    "    elif e1 == 1 and e2 == 1:\n",
    "        locv.append(5)\n",
    "    elif e1 == 1 and e2 == 2:\n",
    "        locv.append(6)\n",
    "    elif e1 == 1 and e2 == 3:\n",
    "        locv.append(7)\n",
    "    elif e1 == 2 and e2 == 0:\n",
    "        locv.append(8)\n",
    "    elif e1 == 2 and e2 == 1:\n",
    "        locv.append(9)\n",
    "    elif e1 == 2 and e2 == 2:\n",
    "        locv.append(10)\n",
    "    elif e1 == 2 and e2 == 3:\n",
    "        locv.append(11)\n",
    "    elif e1 == 2 and e2 == 4:\n",
    "        locv.append(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop floor and building ID feature columns and insert the new combined location feature column into the dataframe\n",
    "df_train = df_train.drop(columns = ['FLOOR', 'BUILDINGID'])\n",
    "df_val = df_val.drop(columns = ['FLOOR', 'BUILDINGID'])\n",
    "\n",
    "df_train['LOCATION'] = loct\n",
    "df_val['LOCATION'] = locv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns that have zero variance across all samples, i.e. the same value for every sample in the training set\n",
    "n = df_train.nunique(axis = 0)\n",
    "dropcol = n[n == 1].index\n",
    "df_train = df_train.drop(dropcol, axis = 1)\n",
    "\n",
    "#drop the same columns from the validation set\n",
    "df_val = df_val.drop(dropcol, axis = 1)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the WAP columns as features and the location column as the target\n",
    "xtrain = df_train.loc[:, 'WAP001' : 'WAP519']\n",
    "ytrain = df_train.loc[:, 'LOCATION']\n",
    "\n",
    "xval = df_val.loc[:, 'WAP001' : 'WAP519']\n",
    "yval = df_val.loc[:, 'LOCATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to fit model without normalizing the data\n",
    "#this was the first model I tried but after trying a fourth, this one had the least accuracy.\n",
    "#I kept the code here anyway.\n",
    "#lsvc = LinearSVC(dual = False)\n",
    "#lsvc.fit(xtrain, ytrain)\n",
    "#ylsvc = lsvc.predict(xval)\n",
    "#accuracy_score(yval, ylsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to fit model without normalizing the data\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(xtrain, ytrain)\n",
    "yknn = knn.predict(xval)\n",
    "accuracy_score(yval, yknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to fit model without normalizing the data\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(xtrain, ytrain)\n",
    "yrf = rf.predict(xval)\n",
    "accuracy_score(yval, yrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt to fit model without normalizing the data\n",
    "svc = svm.SVC()\n",
    "svc.fit(xtrain, ytrain)\n",
    "ysvc = svc.predict(xval)\n",
    "accuracy_score(yval, ysvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "#Confirm the maximum and minimum value of the selected features and if there are values measured at 0, i.e. the strongest signal\n",
    "print(xtrain.min().min())\n",
    "print(xtrain.max().max())\n",
    "strongest = 0 in xtrain.values\n",
    "print(strongest)\n",
    "xtrain.describe()\n",
    "\n",
    "#I replaced the undetected WAP values with -105 so they would represent the weakest signal.\n",
    "#Then I normalized the data bringing the values between 0 and 1 with 0 being the weakest and 1 being the strongest.\n",
    "xtrain = xtrain.replace(100, -105)\n",
    "xtrain = 1 - (xtrain/-105)\n",
    "\n",
    "xval = xval.replace(100, -105)\n",
    "xval = 1 - (xval/-105)\n",
    "\n",
    "xtrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was the first model I tried but after trying a fourth, this one had the least accuracy.\n",
    "#I kept the code here anyway.\n",
    "#lsvc = LinearSVC(dual = False)\n",
    "#lsvc.fit(xtrain, ytrain)\n",
    "#ylsvc = lsvc.predict(xval)\n",
    "#accuracy_score(yval, ylsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(xtrain, ytrain)\n",
    "yknn = knn.predict(xval)\n",
    "accuracy_score(yval, yknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(xtrain, ytrain)\n",
    "yrf = rf.predict(xval)\n",
    "accuracy_score(yval, yrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC()\n",
    "svc.fit(xtrain, ytrain)\n",
    "ysvc = svc.predict(xval)\n",
    "a100 = accuracy_score(yval, ysvc)\n",
    "print(a100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC was the best performing model\n",
    "#visualize the results\n",
    "cm = confusion_matrix(yval, ysvc)\n",
    "cmp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels= svc.classes_)\n",
    "cmp.plot()\n",
    "plt.show()\n",
    "print(classification_report(yval,ysvc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I normalized the features on the xtrain dataframe so I am updating df_train with the scaled features so I can use df_train to get sample training sets.\n",
    "df_train.loc[:,'WAP001':'WAP519'] = xtrain\n",
    "\n",
    "#getting three different sets of 20% of the training set\n",
    "train20_1 = df_train.sample(n = int(.2*len(df_train)))\n",
    "train20_2 = df_train.sample(n = int(.2*len(df_train)))\n",
    "train20_3 = df_train.sample(n = int(.2*len(df_train)))\n",
    "x20_1 = train20_1.loc[:,'WAP001':'WAP519']\n",
    "y20_1 = train20_1.loc[:,'LOCATION']\n",
    "x20_2 = train20_2.loc[:,'WAP001':'WAP519']\n",
    "y20_2 = train20_2.loc[:,'LOCATION']\n",
    "x20_3 = train20_3.loc[:,'WAP001':'WAP519']\n",
    "y20_3 = train20_3.loc[:,'LOCATION']\n",
    "\n",
    "#run model on each set\n",
    "svc.fit(x20_1, y20_1)\n",
    "predict20_1 = svc.predict(xval)\n",
    "a20_1 = accuracy_score(yval, predict20_1)\n",
    "print(a20_1)\n",
    "svc.fit(x20_2, y20_2)\n",
    "predict20_2 = svc.predict(xval)\n",
    "a20_2 = accuracy_score(yval, predict20_2)\n",
    "print(a20_2)\n",
    "svc.fit(x20_3, y20_3)\n",
    "predict20_3 = svc.predict(xval)\n",
    "a20_3 = accuracy_score(yval, predict20_3)\n",
    "print(a20_3)\n",
    "\n",
    "#average accuracy of the three experiments\n",
    "a20 = (a20_1 + a20_2 + a20_3)/3\n",
    "print(a20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting three different sets of 40% of the training set\n",
    "train40_1 = df_train.sample(n = int(.4*len(df_train)))\n",
    "train40_2 = df_train.sample(n = int(.4*len(df_train)))\n",
    "train40_3 = df_train.sample(n = int(.4*len(df_train)))\n",
    "x40_1 = train40_1.loc[:,'WAP001':'WAP519']\n",
    "y40_1 = train40_1.loc[:,'LOCATION']\n",
    "x40_2 = train40_2.loc[:,'WAP001':'WAP519']\n",
    "y40_2 = train40_2.loc[:,'LOCATION']\n",
    "x40_3 = train40_3.loc[:,'WAP001':'WAP519']\n",
    "y40_3 = train40_3.loc[:,'LOCATION']\n",
    "\n",
    "#run model on each set\n",
    "svc.fit(x40_1, y40_1)\n",
    "predict40_1 = svc.predict(xval)\n",
    "a40_1 = accuracy_score(yval, predict40_1)\n",
    "print(a40_1)\n",
    "svc.fit(x40_2, y40_2)\n",
    "predict40_2 = svc.predict(xval)\n",
    "a40_2 = accuracy_score(yval, predict40_2)\n",
    "print(a40_2)\n",
    "svc.fit(x40_3, y40_3)\n",
    "predict40_3 = svc.predict(xval)\n",
    "a40_3 = accuracy_score(yval, predict40_3)\n",
    "print(a40_3)\n",
    "\n",
    "#average accuracy of the three experiments\n",
    "a40 = (a40_1 + a40_2 + a40_3)/3\n",
    "print(a40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting three different sets of 60% of the training set\n",
    "train60_1 = df_train.sample(n = int(.6*len(df_train)))\n",
    "train60_2 = df_train.sample(n = int(.6*len(df_train)))\n",
    "train60_3 = df_train.sample(n = int(.6*len(df_train)))\n",
    "x60_1 = train60_1.loc[:,'WAP001':'WAP519']\n",
    "y60_1 = train60_1.loc[:,'LOCATION']\n",
    "x60_2 = train60_2.loc[:,'WAP001':'WAP519']\n",
    "y60_2 = train60_2.loc[:,'LOCATION']\n",
    "x60_3 = train60_3.loc[:,'WAP001':'WAP519']\n",
    "y60_3 = train60_3.loc[:,'LOCATION']\n",
    "\n",
    "#run model on each set\n",
    "svc.fit(x60_1, y60_1)\n",
    "predict60_1 = svc.predict(xval)\n",
    "a60_1 = accuracy_score(yval, predict60_1)\n",
    "print(a60_1)\n",
    "svc.fit(x60_2, y60_2)\n",
    "predict60_2 = svc.predict(xval)\n",
    "a60_2 = accuracy_score(yval, predict60_2)\n",
    "print(a60_2)\n",
    "svc.fit(x60_3, y60_3)\n",
    "predict60_3 = svc.predict(xval)\n",
    "a60_3 = accuracy_score(yval, predict60_3)\n",
    "print(a60_3)\n",
    "\n",
    "#average accuracy of the three experiments\n",
    "a60 = (a60_1 + a60_2 + a60_3)/3\n",
    "print(a60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting three different sets of 80% of the training set\n",
    "train80_1 = df_train.sample(n = int(.8*len(df_train)))\n",
    "train80_2 = df_train.sample(n = int(.8*len(df_train)))\n",
    "train80_3 = df_train.sample(n = int(.8*len(df_train)))\n",
    "x80_1 = train80_1.loc[:,'WAP001':'WAP519']\n",
    "y80_1 = train80_1.loc[:,'LOCATION']\n",
    "x80_2 = train80_2.loc[:,'WAP001':'WAP519']\n",
    "y80_2 = train80_2.loc[:,'LOCATION']\n",
    "x80_3 = train80_3.loc[:,'WAP001':'WAP519']\n",
    "y80_3 = train80_3.loc[:,'LOCATION']\n",
    "\n",
    "#run model on each set\n",
    "svc.fit(x80_1, y80_1)\n",
    "predict80_1 = svc.predict(xval)\n",
    "a80_1 = accuracy_score(yval, predict80_1)\n",
    "print(a80_1)\n",
    "svc.fit(x80_2, y80_2)\n",
    "predict80_2 = svc.predict(xval)\n",
    "a80_2 = accuracy_score(yval, predict80_2)\n",
    "print(a80_2)\n",
    "svc.fit(x80_3, y80_3)\n",
    "predict80_3 = svc.predict(xval)\n",
    "a80_3 = accuracy_score(yval, predict80_3)\n",
    "print(a80_3)\n",
    "\n",
    "#average accuracy of the three experiments\n",
    "a80 = (a80_1 + a80_2 + a80_3)/3\n",
    "print(a80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = ['20%', '40%', '60%', '80%', '100%']\n",
    "yaxis = [a20, a40, a60, a80, a100]\n",
    "plt.scatter(xaxis, yaxis, color = 'pink')\n",
    "plt.ylim(.90, .95, .1)\n",
    "plt.xlabel('Percentage of Training Set')\n",
    "plt.ylabel('Average Model Accuracy')\n",
    "for i in range(len(yaxis)):\n",
    "    plt.annotate(str(yaxis[i])[0:5],(xaxis[i],yaxis[i]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "344443636c3027c5042750c9c609acdda283a9c43681b128a8c1053e7ad2aa7d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
